{
	"id": "1426",
	"name": "Improper Validation of Generative AI Output",
	"abstraction": "Base",
	"structure": "Simple",
	"status": "Incomplete",
	"description": "The product invokes a generative AI/ML\n\t\t\tcomponent whose behaviors and outputs cannot be directly\n\t\t\tcontrolled, but the product does not validate or\n\t\t\tinsufficiently validates the outputs to ensure that they\n\t\t\talign with the intended security, content, or privacy\n\t\t\tpolicy.",
	"related_weaknesses": [
		{
			"nature": "ChildOf",
			"cweid": "707",
			"view_id": "1000",
			"ordinal": "Primary"
		}
	],
	"applicable_platforms": {
		"language": [
			{
				"class": "Not Language-Specific",
				"prevalence": "Undetermined"
			}
		],
		"technology": [
			{
				"prevalence": "Undetermined",
				"name": "AI/ML"
			},
			{
				"class": "Not Technology-Specific",
				"prevalence": "Undetermined"
			}
		],
		"architecture": [
			{
				"class": "Not Architecture-Specific",
				"prevalence": "Undetermined"
			}
		]
	},
	"modes_of_introduction": [
		{
			"phase": "Architecture and Design",
			"note": [
				"<xhtml:p>Developers may rely heavily on protection mechanisms such as\ninput filtering and model alignment, assuming they are more effective\nthan they actually are.</xhtml:p>\n\t\t\t\t\t"
			]
		},
		{
			"phase": "Implementation",
			"note": [
				"<xhtml:p>Developers may rely heavily on protection mechanisms such as\ninput filtering and model alignment, assuming they are more effective\nthan they actually are.</xhtml:p>\n\t\t\t\t\t"
			]
		}
	],
	"common_consequences": [
		{
			"scope": [
				"Integrity"
			],
			"impact": [
				"Execute Unauthorized Code or Commands",
				"Varies by Context"
			],
			"note": "\n\t\t\t\t\t"
		}
	],
	"potential_mitigations": [
		{
			"phase": [
				"Architecture and Design"
			],
			"description": [
				"Since the output from a generative AI component (such as an LLM) cannot be trusted, ensure that it operates in an untrusted or non-privileged space."
			]
		},
		{
			"phase": [
				"Operation"
			],
			"description": [
				"Use \"semantic comparators,\" which are mechanisms that\n\t\t\t\t\tprovide semantic comparison to identify objects that might appear\n\t\t\t\t\tdifferent but are semantically similar."
			]
		},
		{
			"phase": [
				"Operation"
			],
			"description": [
				"<xhtml:p>Use components that operate\n\t\t\t\t\texternally to the system to monitor the output and\n\t\t\t\t\tact as a moderator. These components are called\n\t\t\t\t\tdifferent terms, such as supervisors or\n\t\t\t\t\tguardrails.</xhtml:p>"
			]
		},
		{
			"phase": [
				"Build and Compilation"
			],
			"description": [
				"<xhtml:p>During model training, use an appropriate variety of good\n\t\t\t\t  and bad examples to guide preferred outputs.</xhtml:p>"
			]
		}
	],
	"observed_examples": [
		{
			"reference": "CVE-2024-3402",
			"description": "chain: GUI for ChatGPT API performs\n\t\t\t\t\tinput validation but does not properly \"sanitize\"\n\t\t\t\t\tor validate model output data (CWE-1426), leading\n\t\t\t\t\tto XSS (CWE-79).",
			"link": "https://www.cve.org/CVERecord?id=CVE-2024-3402"
		}
	],
	"references": [
		{
			"reference_id": "REF-1441",
			"author": [
				"OWASP"
			],
			"title": "LLM02: Insecure Output Handling",
			"url": "https://genai.owasp.org/llmrisk/llm02-insecure-output-handling/",
			"publication_year": "2024",
			"publication_month": "--03",
			"publication_day": "---21",
			"url_date": "2024-07-11"
		},
		{
			"reference_id": "REF-1442",
			"author": [
				"Cohere",
				"Guardrails AI"
			],
			"title": "Validating Outputs",
			"url": "https://cohere.com/blog/validating-llm-outputs",
			"publication_year": "2023",
			"publication_month": "--09",
			"publication_day": "---13",
			"url_date": "2024-07-11"
		},
		{
			"reference_id": "REF-1443",
			"author": [
				"Traian Rebedea",
				"Razvan Dinu",
				"Makesh Sreedhar",
				"Christopher Parisien",
				"Jonathan Cohen"
			],
			"title": "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails",
			"url": "https://aclanthology.org/2023.emnlp-demo.40/",
			"publication_year": "2023",
			"publication_month": "--12",
			"url_date": "2024-07-11"
		},
		{
			"reference_id": "REF-1444",
			"author": [
				"Snyk"
			],
			"title": "Insecure output handling in LLMs",
			"url": "https://learn.snyk.io/lesson/insecure-input-handling/",
			"url_date": "2024-07-11"
		},
		{
			"reference_id": "REF-1445",
			"author": [
				"Yi Dong",
				"Ronghui Mu",
				"Gaojie Jin",
				"Yi Qi",
				"Jinwei Hu",
				"Xingyu Zhao",
				"Jie Meng",
				"Wenjie Ruan",
				"Xiaowei Huang"
			],
			"title": "Building Guardrails for Large Language Models",
			"url": "https://arxiv.org/pdf/2402.01822",
			"publication_year": "2024",
			"publication_month": "--05",
			"publication_day": "---29",
			"url_date": "2024-07-11"
		}
	],
	"content_history": {
		"submission": {
			"submission_name": "Members of the CWE AI WG",
			"submission_organization": "CWE Artificial Intelligence (AI) Working Group (WG)",
			"submission_date": "2024-07-02"
		}
	},
	"detection_methods": [
		{
			"method": "Dynamic Analysis with Manual Results Interpretation",
			"description": "Use known techniques for prompt injection\n\t\t\t and other attacks, and adjust the attacks to be more\n\t\t\t specific to the model or system."
		},
		{
			"method": "Dynamic Analysis with Automated Results Interpretation",
			"description": "Use known techniques for prompt injection\n\t\t\t and other attacks, and adjust the attacks to be more\n\t\t\t specific to the model or system."
		},
		{
			"method": "Architecture or Design Review",
			"description": "Review of the product design can be\n\t\t\t effective, but it works best in conjunction with dynamic\n\t\t\t analysis."
		}
	],
	"notes": [
		{
			"type": "Research Gap",
			"text": "\n\t\t\t This entry is related to AI/ML, which is not well\n\t\t\t understood from a weakness perspective. Typically, for\n\t\t\t new/emerging technologies including AI/ML, early\n\t\t\t vulnerability discovery and research does not focus on\n\t\t\t root cause analysis (i.e., weakness identification). For\n\t\t\t AI/ML, the recent focus has been on attacks and\n\t\t\t exploitation methods, technical impacts, and mitigations.\n\t\t\t As a result, closer research or focused efforts by SMEs\n\t\t\t is necessary to understand the underlying weaknesses.\n\t\t\t Diverse and dynamic terminology and rapidly-evolving\n\t\t\t technology further complicate understanding. Finally,\n\t\t\t there might not be enough real-world examples with\n\t\t\t sufficient details from which weakness patterns may be\n\t\t\t discovered. For example, many real-world vulnerabilities\n\t\t\t related to \"prompt injection\" appear to be related to\n\t\t\t typical injection-style attacks in which the only\n\t\t\t difference is that the \"input\" to the vulnerable\n\t\t\t component comes from model output instead of direct\n\t\t\t adversary input, similar to \"second-order SQL injection\"\n\t\t\t attacks."
		},
		{
			"type": "Maintenance",
			"text": "This entry was created by members\n           of the CWE AI Working Group during June and July 2024. The\n           CWE Project Lead, CWE Technical Lead, AI WG co-chairs, and\n           many WG members decided that for purposes of timeliness, it\n           would be more helpful to the CWE community to publish the\n           new entry in CWE 4.15 quickly and add to it in subsequent\n           versions."
		}
	]
}
